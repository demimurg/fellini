{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "class Lang(Enum):\n",
    "    EN = 1\n",
    "    RU = 2\n",
    "\n",
    "# choose language (affect movie dataset)\n",
    "ENGINE = Lang.RU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solve problem with mystem package in google colab\n",
    "!if [[ \"$OSTYPE\" == \"linux-gnu\" ]]; then wget http://download.cdn.yandex.net/mystem/mystem-3.0-linux3.1-64bit.tar.gz && tar -xvf mystem-3.0-linux3.1-64bit.tar.gz && cp mystem /bin; fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import requests\n",
    "import pandas as pd\n",
    "from pymystem3 import Mystem\n",
    "\n",
    "COLAB_ENV = \"google.colab\" in str(get_ipython())\n",
    "ENV_PREFIX = \"https://raw.githubusercontent.com/madmaxeatfax/fellini/master/datasets/\" if COLAB_ENV else \"./datasets/\"\n",
    "\n",
    "\n",
    "def get_stopwords(lang):\n",
    "    data, path = None, ENV_PREFIX + f\"stopwords_{lang}.txt\"\n",
    "    if COLAB_ENV:\n",
    "        data = requests.get(path).text.split(\"\\n\")\n",
    "    else:\n",
    "        data = open(path)\n",
    "\n",
    "    stopwords = set()\n",
    "    for word in data:\n",
    "        stopwords.add(word.rstrip())\n",
    "\n",
    "    return stopwords\n",
    "\n",
    "\n",
    "class EnLemmaTokenizer:\n",
    "    def  __init__(self):\n",
    "        self.nlp = spacy.load(\"en\")\n",
    "        self.stopwords = get_stopwords(\"en\")\n",
    "    def __call__(self, text):\n",
    "        return [\n",
    "            token.lemma_ for token in self.nlp(text)\n",
    "            if token.lemma_ not in self.stopwords \n",
    "            and token.lemma_[0].isalpha()\n",
    "        ]\n",
    "\n",
    "class RuLemmaTokenizer:\n",
    "    def  __init__(self):\n",
    "        self.lemma = Mystem().lemmatize\n",
    "        self.stopwords = get_stopwords(\"ru\")\n",
    "    def __call__(self, text):\n",
    "        return [\n",
    "            token for token in self.lemma(text)\n",
    "            if token not in self.stopwords and token[0].isalpha()\n",
    "        ]\n",
    "\n",
    "lemma_tokenizer = EnLemmaTokenizer() if ENGINE == Lang.EN else RuLemmaTokenizer()\n",
    "\n",
    "bunch = pd.read_csv(ENV_PREFIX + (\"imdb250.csv\" if ENGINE == Lang.EN else \"kp250.csv\"))\n",
    "bunch.fillna(\"\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vec = TfidfVectorizer(max_features=50_000, tokenizer=lemma_tokenizer)\n",
    "data = \\\n",
    "    bunch.Title + \" \" + bunch.Crew + \" \" + bunch.Plot + \" \" + \\\n",
    "    bunch.Tags + \" \" + bunch.Country + \" \" + bunch.Reviews\n",
    "\n",
    "features = vec.fit_transform(data)\n",
    "\n",
    "words = vec.get_feature_names()\n",
    "print(f'Dictionary size = {len(words)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "knn = NearestNeighbors(n_neighbors=5, metric='cosine')\n",
    "knn.fit(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# write your search query\n",
    "QUERY = \"фильм в котором бухгалтер убил свою жену и отправился в тюрьму\"\n",
    "query_vector = vec.transform([QUERY])\n",
    "print(f'Query = \\'{QUERY}\\'\\nTokens =  {lemma_tokenizer(QUERY)}\\n')\n",
    "\n",
    "distances, neighbors = knn.kneighbors(query_vector, return_distance=True)\n",
    "    \n",
    "for dist, neighbor_idx in zip(distances[0], neighbors[0]):\n",
    "    print(*[\n",
    "        bunch.Title[neighbor_idx],\n",
    "        f'Distance = {dist}  Neighbor idx = {neighbor_idx}',\n",
    "        bunch.Plot[neighbor_idx][:200], \n",
    "        bunch.Crew[neighbor_idx],\n",
    "        \"-\"*200\n",
    "    ], sep=\"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.0 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "ac2eaa0ea0ebeafcc7822e65e46aa9d4f966f30b695406963e145ea4a91cd4fc"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}